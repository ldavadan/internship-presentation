---
title: "Vers une interpolation de la température avec la Régression Linéaire Multiple"
author: "loic.davadan@agro-bordeaux.fr"
date: "`r format(Sys.time(), '%d %B, %Y')`"
always_allow_html: yes
output:
  revealjs::revealjs_presentation:
    css:
      - "~/Documents/code/internship-presentation/theme/style_craw.css"
    center: no
    highlight: zenburn
    incremental: yes
    self_contained: true
    slide_level: 2
    theme: default
    transition: slide
    fig_caption: yes
    reveal_options:
      previewLinks: false
  md_document:
    toc: no
    toc_depth: 6
    variant: markdown_github
  word_document:
    toc: no
    toc_depth: '6'
  odt_document:
    fig_height: 5
    fig_width: 7
  pdf_document: default
  html_document:
    theme: default
    toc: yes
    toc_depth: 6
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r include=FALSE}
load("~/Documents/code/internship-presentation/data/env_data_presentation.RData")
library(plotly)
library(mlr)
```

# 1. Objectifs
##
* Spatialiser des données de températures prédites à l'aide de la Régression Linéaire Multiple en vue d'une comparaison avec d'autres méthodes statistiques (krigeage, réseaux neuronaux...) pour évaluer la meilleure méthode.

* Intégrer des variables explicatives et trouver la meilleure combinaison de ces variables fournissant des prédictions avec une RMSE faible.

# 2. Travaux réalisés
##
Préparation de l'environnement de travail

* Installation de Linux : Open-source, science reproductible

* Utilisation de Docker : Environnement de travail isolé avec configuration différente de l'ordinateur hôte. Assure la reproductibilité du code

* Utilisation de GitHub : Service de gestion de versions -> Travail collaboratif

## 2.1 Récolte des données   

### 2.1.1. Variables explicatives

#### 2.1.1.1 Variables statiques

Données d'occupation du sol (CORINE Land Cover)

Sources : [Copernicus](https://land.copernicus.eu/pan-european/corine-land-cover/view) et [Géoportail belge](https://www.geo.be/#!/catalog/details/bcd19aa9-c320-4116-971b-6e4376137f13?l=en)

* Stations en zones agricoles mais environnement à caractériser (route en asphalte à proximité ? forêt faisant de l'ombre ?)


* 47 classes dont 26 présentes en Wallonie -> regroupement en 5 classes : Zones artificielles, Zones de cultures, Zones herbacées, Forêt et Plans d'eau

* Récupération des données aux stations : zones tampons (100m autour des stations physiques et 500m pour les stations virtuelles) et calcul de la part de chaque classe dans l'environnement de la station

----

```{r table, echo=FALSE, fig.width=6, fig.height=6, fig.cap = "Exemple de résultat"}
head(class.buff.clean.df)
```


Part de chaque classe autour de chaque station virtuelle

----

![Récupération des données aux stations virtuelles avec buffers](./data/CLC_buffer.png)
Récupération des données aux stations virtuelles avec buffers


----

Modèle Numérique de Terrain

Source : [SRTM](https://lta.cr.usgs.gov/SRTM)

* Caractériser l'effet de la topographie du milieu sur la température

* Données d'altitude (résolution 90m) puis calcul de la pente, l'orientation et la rugosité du terrain avec R

* Données très volumineuses = Temps de calcul important

----

### 2.1.1.2. Variables dynamiques

Source : API ensoleillement

* Caractériser l'impact du rayonnement solaire (W/m²) sur la température

* Rayonnement solaire : données [EUMETSAT](https://landsaf.ipma.pt/en/products/longwave-shortwave-radiation/dssf/) récupérées sur une API

* Données horaires, 875 points répartis sur la Wallonie -> insuffisant pour la précision souhaitée d'1 km² (~ 17000 points)

* Spatialisation des données solaires à l'aide d'une méthode de krigeage (interpolation linéaire)

* Récupération des données d'ensoleillement des stations PAMESEB en parallèle

## 2.1.2. Variables d'intérêt

Source : API AGROMET

* Besoin de données pour construire les modèles et comparer les outputs à la réalité

* 29 stations du réseau PAMESEB

* Accès à des données dynamiques horaires : __température__, humidité relative, humectation du feuillage

* 27 stations utilisables si on prend l'ensoleillement

## 2.2. Organisation des données

_Objectif_ : construire des modèles pour chaque heure

Regroupement des données statiques et dynamiques de chaque station.

Pour optimiser le temps de calcul et l'intégration dans `mlr`, création d'un "nested data frame" : chaque heure contient un ou plusieurs tableaux de données

![](./data/purrr_nest.png){width=30%}

## 2.3. Modélisation

### 2.3.1. Machine learning

_Définition_ : Le Machine Learning est un concept stipulant qu’il existe des algorithmes génériques pouvant nous révéler des informations intéressantes sur des données, sans avoir besoin de construire ou de développer un code spécifique. Au lieu d’écrire du code, vous nourrissez donc ces algorithmes avec des données qui leur permettront de construire leurs propres logiques.

_Objectif_ : Réaliser un benchmark appliquant la Régression Linéaire Multiple à différentes combinaisons de variables explicatives et au paramètre ciblé en utilisant une stratégie de rééchantillonage par validation croisée (LOOCV).

----

### 2.3.2. Machine Learning in R

`mlr` : package R proposant une interface simplifiée et commune pour toutes les méthodes statistiques à intégrer dans le machine learning.

Paramètres à définir pour la modélisation :

* Le paramètre ciblé : Température

* Les méthodes statistiques d'apprentissage : Régression linéaire multiple

* Les variables explicatives à comparer : MNT, occupation du sol, ensoleillement

* La méthode de rééchantillonage : validation croisée par Leave-One-Out


# 3. Résultats

## 3.1. Visualisation du modèle 

* Graphiques des résultats du benchmark

* Equation de régression des modèles et leurs coefficients

* Cartes : Statiques ou interactives (leaflet) & Visualisation des prédictions pour une heure donnée mais aussi de l'erreur relative

* Création d'une couche contenant les prédictions, visualisée avec une palette de couleur

* Création d'une couche contenant l'erreur associée à chaque prédiction avec un niveau de transparence variable (couche blanche avec opacité plus importante si l'erreur est élevée)

## 3.2. Premiers résultats

Output pour une heure après modélisation (benchmark réalisé sur 2 mois)

![](./data/template_map.png){width=60%}

Output avec pour combinaison de variables __ensoleillement + meilleure variable explicative (corrélation linéaire)__, erreur en niveaux de transparence

(inspiré par [Visual Representations of Data Uncertainty](https://www.e-education.psu.edu/geog486/node/1891) et [Transparency and Alpha levels for ggplot2](https://stackoverflow.com/questions/21193138/transparency-and-alpha-levels-for-ggplot2-stat-density2d-with-maps-and-layers-in))

----

```{r echo=FALSE}
ggplotly(plotBMRSummary(bmr.w.l, pretty.names = F, pointsize = 1))
```

Comparaison des modèles construits à partir de l'altitude et/ou de l'ensoleillement

# 4. Perspectives et conclusions
##
La démarche est définie et prête pour créer les modèles.

_Prochains objectifs_ :

* Créer les modèles à partir de 5 ans de données

* Identifier la meilleure combinaison de variables explicatives

* Appliquer cette combinaison à la grille d'interpolation

* Créer une _Shiny app_ pour la visualisation des données



